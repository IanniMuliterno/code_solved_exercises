{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spacy vs nltk tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "import re\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "Ian is a king. I like studying monarchy. the most problematic politicians are those who rules for themselves. There is a Dr. Joe Dispenza, who is so freaking inspiring\n",
    "'''\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ian is a king.\n",
      "I like studying monarchy.\n",
      "the most problematic politicians are those who rules for themselves.\n",
      "There is a Dr. Joe Dispenza, who is so freaking inspiring\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sentence tokenization\n",
    "for token in doc.sents:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Ian\n",
      "is\n",
      "a\n",
      "king\n",
      ".\n",
      "I\n",
      "like\n",
      "studying\n",
      "monarchy\n",
      ".\n",
      "the\n",
      "most\n",
      "problematic\n",
      "politicians\n",
      "are\n",
      "those\n",
      "who\n",
      "rules\n",
      "for\n",
      "themselves\n",
      ".\n",
      "There\n",
      "is\n",
      "a\n",
      "Dr.\n",
      "Joe\n",
      "Dispenza\n",
      ",\n",
      "who\n",
      "is\n",
      "so\n",
      "freaking\n",
      "inspiring\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#word tokenization\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ian',\n",
       " 'is',\n",
       " 'a',\n",
       " 'king',\n",
       " '.',\n",
       " 'I',\n",
       " 'like',\n",
       " 'studying',\n",
       " 'monarchy',\n",
       " '.',\n",
       " 'the',\n",
       " 'most',\n",
       " 'problematic',\n",
       " 'politicians',\n",
       " 'are',\n",
       " 'those',\n",
       " 'who',\n",
       " 'rules',\n",
       " 'for',\n",
       " 'themselves']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#word tokenization\n",
    "nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nIan is a king.',\n",
       " 'I like studying monarchy.',\n",
       " 'the most problematic politicians are those who rules for themselves.',\n",
       " 'There is a Dr. Joe Dispenza, who is so freaking inspiring']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sentence tokenization\n",
    "nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Think stats is a free book to study statistics (https://greenteapress.com/thinkstats2/thinkstats2.pdf)\n",
    "\n",
    "This book has references to many websites from where you can download free datasets. You are an NLP engineer working for some company and you want to collect all dataset websites from this book. To keep exercise simple you are given a paragraph from this book and you want to grab all urls from this paragraph using spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text='''\n",
    "Look for data to help you address the question. Governments are good\n",
    "sources because data from public research is often freely available. Good\n",
    "places to start include http://www.data.gov/, and http://www.science.\n",
    "gov/, and in the United Kingdom, http://data.gov.uk/.\n",
    "Two of my favorite data sets are the General Social Survey at http://www3.norc.org/gss+website/, \n",
    "and the European Social Survey at http://www.europeansocialsurvey.org/.\n",
    "'''\n",
    "\n",
    "# TODO: Write code here\n",
    "# Hint: token has an attribute that can be used to detect a url\n",
    "doc = nlp(text.replace('\\n',''))\n",
    "sites = [token.text for token in doc if token.like_url]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://www.data.gov/', 'http://www.science.gov/', 'http://data.gov.uk/', 'http://www3.norc.org/gss+website/', 'http://www.europeansocialsurvey.org/']\n"
     ]
    }
   ],
   "source": [
    "def extract_before_last_slash(input_string):\n",
    "    if not isinstance(input_string, str):\n",
    "        raise ValueError(\"Expected a string as input\")\n",
    "    \n",
    "    # Regular expression pattern\n",
    "    pattern = r\".*/\"\n",
    "\n",
    "    # Find all matches\n",
    "    matches = re.findall(pattern, input_string)\n",
    "\n",
    "    # Return the last match (if any)\n",
    "    if matches:\n",
    "        return matches[-1]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "results = [extract_before_last_slash(site) for site in sites]\n",
    "\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Extract all money transaction from below sentence along with currency. Output should be,\n",
    "\n",
    "two $\n",
    "\n",
    "500 €"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two $\n",
      "500 €\n"
     ]
    }
   ],
   "source": [
    "transactions = \"Tony gave two $ to Peter, Bruce gave 500 € to Steve\"\n",
    "\n",
    "# TODO: Write code here\n",
    "# Hint: Use token.i for the index of a token and token.is_currency for currency symbol detection\n",
    "doc = nlp(transactions)\n",
    "\n",
    "for token in doc:\n",
    "    if token.is_currency:\n",
    "        output = f'{doc[(token.i - 1)]} {token}'\n",
    "        print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: Write a Python function to tokenize a given sentence into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['este', 'e', 'um', 'teste']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(sentence):\n",
    "    return sentence.split()\n",
    "\n",
    "def tokenize_spacy(sentence):\n",
    "    doc  = nlp(sentence)\n",
    "    return [token.text for token in doc]\n",
    "\n",
    "tokenize_spacy('este e um teste')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['este', 'e', 'um', 'teste']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize('este e um teste')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: Write a Python function to remove stop words from a given list of words using the nltk and spacy libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: Write a Python function to perform stemming on a list of words using the nltk library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: Write a Python function to perform named entity recognition (NER) on a given sentence using the spacy library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
